\documentclass{beamer}

\usepackage{multirow}

\newcommand{\ep}{\varepsilon}
\newcommand{\img}[1]{\includegraphics[width=4in]{#1}}

\begin{document}
	
%%%%% Background & Work at Sandia %%%%%
\begin{frame}{Background}
\begin{itemize}
	\item Received Ph.D. in Economics from the University of Oregon in 2017
		\begin{itemize}
			\item Focused on industrial organization and econometrics
			\item Efficiency analyses (technical and allocative)
			\item Structural modeling of pricing and economies of scale
		\end{itemize}
	\item Experience interning at Pacific Northwest National Laboratory
		\begin{itemize}
			\vspace*{-3ex}
			\item Disease modeling
			\item Nuclear proliferation pathway analysis
			\item Social media analytics
			\item Social network analysis for cyber vulnerabilities
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Projects at Sandia}
\begin{itemize}
	\item Have worked at Sandia National Laboratories since August 2017
	\item Cybersecurity
		\begin{itemize}
			\item Took formal training covering system architectures, common vulnerabilities and attacks, and security measures
			\item Helped develop tool to prioritize engagements by DHS Cybersecurity Advisors
			\item Helped develop tool to estimate resilience costs in real time given network data
			\item Engaged in Information Design Assurance Red Teaming
			\item Participated in industry working-group to update cybersecurity standards for distributed energy resources
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Projects at Sandia}
\begin{itemize}
	\item Statistics
		\begin{itemize}
			\item Reviewed methods of risk analysis of Mars 2020 mission
			\item Developed experimental design and statistical methods for ``laser'' project
		\end{itemize}
	\item Economics
		\begin{itemize}
			\item Analyzed efficiency of technology transfer among national labs
			\item Conducted economic impact analysis of natural and man-made disasters
			\item Studied resiliency of critical infrastructure to natural disasters and investigated potential improvements
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Personal Projects}
\begin{itemize}
	\item Smooth Non-Parametric Frontier Analysis
		\begin{itemize}
			\item Smooth analogue of data envelopment analysis that can be used to estimate technical and allocative efficiency
			\item Developed R package (snfa), which is now available on CRAN
			\item Currently finishing paper on allocative efficiency
		\end{itemize}
	\item Iterative kernel density estimation
		\begin{itemize}
			\item General method to perform Bayesian model selection
			\item Will discuss in more detail later in this talk
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
	\item 
\end{itemize}
\end{frame}


%%%%% Iterative KDE %%%%%	
\begin{frame}{Motivating Example}
\begin{itemize}
	\item Suppose we are trying to fit a production frontier, but have little information about its functional form: $$Y_i = F(X_i)\Delta_i \tilde\ep_i$$
		\begin{itemize}
			\item $Y_i$ is $i$th observation of output
			\item $X_i$ is $i$th observation of inputs
			\item $F$ is (unknown) production function
			\item $\Delta_i\in[0, 1]$ is technical efficiency
			\item $\tilde\ep_i > 0$ is observation error
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Motivating Example: Data}
\begin{figure}
	\centering
	\img{figures/data.pdf}
	\caption{Simulated Data}
\end{figure}
\end{frame}

\begin{frame}{Motivating Example: Cleaning Data}
\begin{itemize}
	\item Common first step: Log-transform: $$y_i = f(X_i) + \delta_i + \ep_i$$
		\begin{itemize}
			\item $y_i = \log Y_i$
			\item $f(X_i) = \log F(X_i)$
			\item $\delta_i = \log \Delta_i \leq 0$
			\item $\ep_i = \log \tilde\ep_i$
		\end{itemize}
	\item Make distributional assumptions:
		\begin{itemize}
			\item $\delta_i\sim N^-(0, \sigma_\delta)$
			\item $\ep_i\sim N(0, \sigma_\ep)$
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Motivating Example: Data}
\begin{figure}
	\centering
	\img{figures/data-log.pdf}
	\caption{Log Simulated Data}
\end{figure}
\end{frame}

\begin{frame}{Motivating Example: Functional Forms}
\begin{itemize}
	\item We still need to select a form for $f$
	\item Traditionally, parametric forms of $f$ have been used
		\begin{itemize}
			\item Log-linear: $f(X_i) = \log(X_i)\beta$
			\item Translog: All log-inputs, squared log-inputs, and interactions between log-inputs
		\end{itemize}
	\item Could also use a non-parametric specification
		\begin{itemize}
			\item Du et al. (2013) use kernel smoothing to fit conditional mean of the data, use residuals to fit distributional parameters
		\end{itemize}
	\item How can we select between these functional forms, especially given limited data?
		\begin{itemize}
			\item Log-linear and translog forms can be compared using many methods
			\item Difficult to compare with non-parametric methods
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Motivating Example: Functional Forms}
\begin{figure}
	\centering
	\img{figures/func-forms.pdf}
	\caption{Fitted Frontiers}
\end{figure}
\end{frame}

\begin{frame}{Model Selection}
\begin{itemize}
	\item Classical methods
		\begin{itemize}
			\item Based on likelihood values
			\item Some methods are restrictive in what models can be compared (e.g., nested models), others like information criteria are general
			\item Require moderate to large sample sizes
			\item \textbf{Problems:}
				\begin{itemize}
					\item Kernel smoothing is not likelihood-based
					\item Sample sizes may not be large enough
					\item Classical methods are unreliable for estimating stochastic frontiers
				\end{itemize}
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Model Selection}
\begin{itemize}
	\item Cross-validation
	\begin{itemize}
		\item Fit the model with one sub-sample, test accuracy against another sub-sample
		\item Requires large sample sizes and a metric for accuracy
		\item \textbf{Problem:} Sample sizes are not large enough
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Model Selection}
\begin{itemize}
	\item Bayesian methods
		\begin{itemize}
			\item Very general model comparison, can estimate the probability a model is correct from a set of exclusive models
				\begin{itemize}
					\item Accounts for likelihood and numbers of parameters
				\end{itemize}
			\item No sample size restrictions in general
			\item \textbf{Difficulties:}
				\begin{itemize}
					\item Need a Bayesian analogue of kernel smoothing (Gaussian processes)
					\item Calculating model probabilities is hard in general, existing methods are unsuitable for large models applied to small samples
				\end{itemize}
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Bayesian Model Selection}
\begin{itemize}
	\item Given a set of exclusive models $\{M_1, ..., M_K\}$, the probability that model $M_k$ is the true model given data $y$ is $$\Pr(M_k|y) = \frac{m(y|M_k)p(M_k)}{\sum_{j=1}^K m(y|M_j)p(M_j)}$$
		\begin{itemize}
			\item $m(y|M_k)$ is marginal likelihood of model $M_k$
			\item $p(M_k)$ is prior probability that $M_k$ is the true model
		\end{itemize}
	\item Marginal likelihoods are difficult to compute in general
\end{itemize}
\end{frame}

\begin{frame}{Marginal Likelihood Estimation}
\begin{itemize}
	\item Many methods have been developed to compute marginal likelihoods
		\begin{itemize}
			\item If Gibbs or Metropolis-Hastings sampling are used, efficient methods developed in Chib (1995) and Chib and Jeliazkov (2001)
			\item Laplace's method and Gaussian quadrature perform well for moderately-sized models
			\item Bridge sampling is very efficient and is widely applicable (in theory)
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Marginal Likelihood Estimation: Problems}
\begin{itemize}
	\item Existing methods are not well suited to marginal likelihood estimation for this model and data
	\begin{itemize}
		\item Model is not of correct form for Gibbs sampling
		\item Excessive convergence times for Metropolis-Hastings sampling
		\item Models have too many parameters relative to sample size for Laplace's method, accuracy is degraded
		\item Models have too many parameters for Gaussian quadrature to be computationally feasible
		\item Bridge sampling is prone to numerical issues in large models, accuracy is degraded (more on this later)
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Numerical Issues With Bridge Sampling}
\begin{itemize}
	\item Bridge sampling estimates marginal likelihood with $$\hat{m}(y) = \frac{\frac{1}{N_2}\sum_{s=1}^{N_2} p\left(y|\theta_{g}^{[s]}\right)p\left(\theta_{g}^{[s]}\right)h\left(\theta_{g}^{[s]}\right)}{\frac{1}{N_1}\sum_{s=1}^{N_1} h\left(\theta_{y}^{[s]}\right)g\left(\theta_{y}^{[s]}\right)}$$
		\begin{itemize}
			\item $\theta$ denote parameters
			\item $p(y|\theta)$ is the likelihood
			\item $p(\theta)$ are priors over parameters
			\item $g(\theta)$ is the proposal distribution, chosen by the researcher
			\item $h(\theta) = \left(r_1 p(y|\theta) p(\theta) + r_2 \hat{m}(y) g(\theta)\right)^{-1}$
			\item $\theta_g$ are parameter samples taken from $g$ (total of $N_2$ samples)
			\item $\theta_y$ are parameter samples taken from the posterior distribution (total of $N_1$ samples)
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Numerical Issues With Bridge Sampling}
$$\hat{m}(y) = \frac{\frac{1}{N_2}\sum_{s=1}^{N_2} p\left(y|\theta_{g}^{[s]}\right)p\left(\theta_{g}^{[s]}\right)h\left(\theta_{g}^{[s]}\right)}{\frac{1}{N_1}\sum_{s=1}^{N_1} h\left(\theta_{y}^{[s]}\right)g\left(\theta_{y}^{[s]}\right)}$$
\begin{itemize}
	\item For large numbers of parameters, $p(\theta)$ and $g(\theta)$ can be very small positive numbers
	\item Thus, $h(\theta)$ can take on very large values
	\item Terms in each sum can be very large or very small, are eventually truncated because of finite machine precision, making numerator and denominator inaccurate
	\item Inaccuracies are magnified by division of numerator by denominator
	\item Will show an example of biased marginal likelihood estimates that result
\end{itemize}
\end{frame}

\begin{frame}{Iterative Kernel Density Estimation}
\begin{itemize}
	\item The marginal likelihood of model $M_k$ can be written as $$m(y|M_k) = \frac{f(y|\theta, M_k)p(\theta|M_k)}{p(\theta|y, M_k)}$$
		\begin{itemize}
			\item $f(y|\theta, M_k)$ is the likelihood, defined by model
			\item $p(\theta)$ is the prior density, defined by researcher
			\item $p(\theta|y, M_k)$ is posterior density, must be estimated
		\end{itemize}
	\item As noted by Chib (1995), this relationship holds for all $\theta$, only need to estimate at one point $\theta^*$, such as the posterior mean
\end{itemize}
\end{frame}

\begin{frame}{Iterative Kernel Density Estimation}
\begin{itemize}
	\item Markov Chain Monte Carlo (MCMC) produces samples of $\theta|y, M_k$, could theoretically use these samples to estimate posterior density
	\item Some problems:
		\begin{itemize}
			\item Traditional kernel density estimators produce biased density estimates
				\begin{itemize}
					\item Traditional estimators overestimate in low density regions and underestimate in high density regions
					\item Adaptive kernel density estimation corrects for this issue (Portnoy and Koenker, 1989)
				\end{itemize}
			\item Kernel density estimation suffers from the curse of dimensionality
				\begin{itemize}
					\item Traditional estimators are largely infeasible for more than six dimensions
					\item Adaptive kernel density estimation is unreliable for more than a few dimensions
				\end{itemize}
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Iterative Kernel Density Estimation}
\begin{itemize}
	\item To address the curse of dimensionality, first denote the parameter vector as $\theta = (\theta_1, \theta_2, ..., \theta_P)'$
	\item Posterior density can be written as
\begin{align}
p(\theta|y)
&= p(\theta_1, ..., \theta_P|y) \\
&= p(\theta_1|\theta_2, ..., \theta_P, y)\times p(\theta_2, ..., \theta_P|y) \\
&= p(\theta_1|\theta_2, ..., \theta_P, y)\times p(\theta_2|\theta_3, ..., \theta_P, y) \\ &\qquad\times p(\theta_3, ..., \theta_P|y) \\
&= ... \\
&= p(\theta_1|\theta_2, ..., \theta_P, y)\times p(\theta_2|\theta_3, ..., \theta_P, y)\\ &\qquad\times ...\times p(\theta_P|y).
\end{align}
	\item The density of a $P$-dimensional vector is broken into $P$ one-dimensional densities
\end{itemize}
\end{frame}

\begin{frame}{Iterative Kernel Density Estimation}
The iterative kernel density estimator has the following algorithm:
\begin{enumerate}
	\item Draw samples of $\theta|y$ using an MCMC algorithm.
	\item Choose $\theta^*$ from a high-density region of $\theta|y$, such as the sample mean or maximum a posteriori.
	\item Estimate the log-density of $\theta_P|y$ at $\theta_P^*$ using adaptive KDE, denoting that value $\ln \hat{p}(\theta_P^*|y)$.
	\item For each $i$ from $P-1, ..., 1$:
	\begin{enumerate}
		\item Re-estimate the model, setting $(\theta_{i+1}, ..., \theta_P) = (\theta_{i+1}^*, ..., \theta_P^*)$, to obtain draws of $(\theta_1, ..., \theta_i)|(\theta_{i+1}^*, ..., \theta_P^*), y$.
		\item Estimate the log-density of $\theta_i|\theta_{i+1}^*, ..., \theta_P^*, y$ at $\theta_i^*$ using adaptive KDE, denoting that value $\ln \hat{p}(\theta_i^*|\theta_{i+1}^*, ..., \theta_P^*, y)$.
	\end{enumerate}
	\item Find the sum of each of the estimated partial log-densities to arrive at an estimate for the overall log-posterior density, denoted $\ln \hat{p}(\theta^*|y)$.
\end{enumerate}
\end{frame}

\begin{frame}{Simulations: Multivariate Normal Linear Model}
\begin{subequations}
	\begin{align}
	y &= X\beta + \ep \\
	\ep &\sim \mbox{ }N(0, \sigma^2 I_{100})\\
	\mbox{\ } \\
	X_1 &= \mathbf{1}_{100}\\
	X_i &\sim U(-10, 10); \mbox{\ \ \ \ } i = 2, 3\\
	\beta &= (-2, 5, 3)'\\
	\sigma &= 25
	\end{align}
\end{subequations}

\begin{itemize}
	\item Marginal likelihood is estimable via Gibbs sampling and with iterative kernel density estimation
		\begin{itemize}
			\item Estimate marginal likelihood using Gibbs sampling and iterative kernel density estimation 500 times each
			\item Test for equality of means
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Simulations: Multivariate Normal Linear Model}
\begin{table}
	\footnotesize
	\centering
	\begin{tabular}{l|c|c|c|c}
		&  &  &  & Mean Test \\ 
		Model & \# Trials & Gibbs/Chib & Iterative KDE & $p$-value \\ 
		\hline
		\hline
		\multirow{3}{*}{Multivariate Linear} & \multirow{3}{*}{500} & -481.353 & -481.348 & \multirow{3}{*}{0.493} \\ 
		&  & (0.154) & (0.078) &  \\ 
		&  & Iter = 5,000 & Iter = 5,000 &  \\ 
		\hline
	\end{tabular}
	\caption{Comparison of Gibbs and Iterative KDE} 
	\label{tab:MVN-Probit}
\end{table}
\end{frame}

\begin{frame}{Simulations: Large Multivariate Normal Linear Model}
\begin{subequations}
	\begin{align}
	y &= X\beta + \ep \\
	\ep &\sim \mbox{ }N(0, \sigma^2 I_{100})\\
	\mbox{\ } \\
	X_1 &= \mathbf{1}_{100}\\
	X_i &\sim U(-10, 10); \mbox{\ \ \ \ } i = 2, ..., 50\\
	\beta_i &\sim U(-10, 10); \mbox{\ \ \ \ } i = 1, ..., 50\\
	\sigma &= 25
	\end{align}
\end{subequations}
\begin{itemize}
	\item Large model has potential to bias bridge sampling estimates
	\item Marginal likelihood estimated with Gibbs sampling, iterative kernel density estimation, and bridge sampling 100 times each
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{table}
	\centering
	\begin{tabular}{c|c|c|c|c}
		& Iterative &  & IKDE = Chib & Bridge = Chib \\ 
		Chib & KDE & Bridge & $p$-value & $p$-value \\ 
		\hline
		\hline
		-606.927 & -606.88 & -607.094 & \multirow{2}{*}{0.125} & \multirow{2}{*}{$1.777\times 10^{-13}$} \\ 
		(0.195) & (0.24) & (0.014) &  &  \\ 
		\hline
	\end{tabular}
	\caption{Comparison of Chib, Iterative KDE, and Bridge Sampling} 
	\label{tab:bridge}
\end{table}
\begin{itemize}
	\item Bias in bridge sampling estimates can result in up to ~5\% difference in model probabilities
\end{itemize}
\end{frame}

\begin{frame}{Other Simulation Results}
\begin{itemize}
	\item Simulations also performed for probit models, showing similar results
	\item Like bridge sampling, Laplace's method, and similar estimators, iterative kernel density estimation is widely applicable
		\begin{itemize}
			\item Used to distinguish probit from logit models in simulations
			\item Used to test different forms of the production function in stochastic frontier model (our motivating example)
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Production Function Selection}
\begin{itemize}
	\item Recall the stochastic frontier model:
		\begin{itemize}
			\item $y_i = \log Y_i$
			\item $f(X_i) = \log F(X_i)$
			\item $\delta_i = \log \Delta_i \leq 0$
			\item $\ep_i = \log \tilde\ep_i$
		\end{itemize}
	\item Distributional assumptions:
		\begin{itemize}
			\item $\delta_i\sim N^-(0, \sigma_\delta)$
			\item $\ep_i\sim N(0, \sigma_\ep)$
		\end{itemize}
	\item Functional forms for $f$:
		\begin{itemize}
			\item Log-linear
			\item Translog
			\item Non-parametric
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Production Function Selection: Data}
\begin{itemize}
	\item Sample dataset describing cereal production of 29 countries in 2012, from the World Bank
	\item Output: Metric tons of cereal grains produced
	\item Inputs: Area of land used for cereal production, fertilizer consumption, freshwater withdrawals, average rainfall, total number of people working in agriculture
\end{itemize}
\end{frame}

\begin{frame}{Production Function Selection}
\begin{itemize}
	\item Gibbs sampling can be used to estimate log-linear and translog forms
	\item Non-parametric (Gaussian process) form must be estimated using another method
	\item Non-parametric form has many parameters, data are relatively limited
		\begin{itemize}
			\item Iterative kernel density estimation is the only method appropriate for estimating marginal likelihood in this example
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Production Function Selection: Results}
\begin{table}[!h]
	\footnotesize
	\centering
	\begin{tabular}{l|c|c|c|c|c}
		&  & Log & Log & Log & Log Marginal \\ 
		Model & \# Parameters & Likelihood & Prior & Posterior & Likelihood \\ 
		\hline
		\hline
		Log-Linear & 8 & -3.734 & -10.225 & 15.249 & -29.208 \\ 
		Translog & 23 & -0.699 & -23.666 & 44.62 & -68.984 \\ 
		GP & 37 & 2.68 & -27.937 & 21.458 & -46.716 \\ 
		\hline
	\end{tabular}
	\caption{Marginal Likelihood Estimates} 
	\label{tab:GPSF-ML}
\end{table}
\begin{itemize}
	\item 
\end{itemize}
\end{frame}

\begin{frame}{}
\begin{itemize}
	\item 
\end{itemize}
\end{frame}

\end{document}
